{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6dfe123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, HiveContext, Row, functions as F\n",
    "from pyspark.sql.functions import avg, count, max, min, udf, substring, expr, concat, col, concat_ws\n",
    "from pyspark.sql.types import FloatType, BooleanType, DateType\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17253a97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/29 15:37:39 WARN Utils: Your hostname, muhammad-Vm resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/10/29 15:37:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/29 15:37:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create the spark Session\n",
    "spark = SparkSession.builder.appName(\"sentiment_load\").config(\"spark.driver.extraClassPath\",\"/usr/local/spark/jars/mysql-connector-java-8.0.33.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67795e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sentiment analysis function to be applied to the tweets\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model = \"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a56ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user defined fucntion (UDF) to apply the sentiment analysis to the select text of the tweets df\n",
    "def get_sentiment_score(text):\n",
    "    result = sentiment_analyzer(text)\n",
    "    score = result[0][\"score\"]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d16f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user defined fucntion (UDF) to create a new date column with an abbreviated value\n",
    "def extract_and_concat(input_string):\n",
    "    first_5 = input_string[:10]\n",
    "    last_4 = input_string[-4:]\n",
    "    return f\"{first_5} {last_4}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cb4e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extract_and_concat(input_string)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register the UDF's with the current spark session\n",
    "spark.udf.register(\"get_sentiment_score\", get_sentiment_score)\n",
    "spark.udf.register(\"extract_and_concat\", extract_and_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c571a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the MySQL connection settings\n",
    "jdbc_url = \"jdbc:mysql://localhost:3306/Tweets\"\n",
    "connection_properties = {\n",
    "    \"user\":\"root\",\n",
    "    \"password\":\"password\",\n",
    "    \"driver\":\"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c4dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the spark df by reading the data from the MySWL table\n",
    "df = spark.read.jdbc(\n",
    "    url = jdbc_url, \n",
    "    table = \"tweetable1\", \n",
    "    properties = connection_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b467a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sentiment analysis UDF to the text column, and create a new column \"sentiment_score\", with the results\n",
    "df = df.withColumn(\"sentiment_score\", udf(get_sentiment_score)(df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b8b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the date abbreviation UDF to the date column, and create a new column \"full_date\", with the results\n",
    "df = df.withColumn(\"full_date\", udf(extract_and_concat)(df[\"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d00b294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringType()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the sentiment_score column data type\n",
    "df.schema[\"sentiment_score\"].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f98d5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dt from string to float\n",
    "df = df.withColumn(\"sentiment_score\", col(\"sentiment_score\").cast(\"Float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ef0ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seperate df with just the full_date and sentiment_score columns neccessary for the time series sentiment analysis.\n",
    "df1 = df.select(\"full_date\", \"sentiment_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eb1aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally write to a csv file for backup/later use\n",
    "#df1.write.format(\"csv\").mode(\"overwrite\").save(\"/home/hduser/Downloads/testsheet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a2d658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert the pyspark df to a pandas df\n",
    "pandas_df = df1.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8717ab22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_date</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon Apr 06 2009</td>\n",
       "      <td>0.998989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Apr 06 2009</td>\n",
       "      <td>0.982745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Apr 06 2009</td>\n",
       "      <td>0.976971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon Apr 06 2009</td>\n",
       "      <td>0.996757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Apr 06 2009</td>\n",
       "      <td>0.996205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         full_date  sentiment_score\n",
       "0  Mon Apr 06 2009         0.998989\n",
       "1  Mon Apr 06 2009         0.982745\n",
       "2  Mon Apr 06 2009         0.976971\n",
       "3  Mon Apr 06 2009         0.996757\n",
       "4  Mon Apr 06 2009         0.996205"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the top 10 rows\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaca8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the current spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8214305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new spark session for connection to Apache Hive\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"sentiment_write\"\n",
    ").config(\n",
    "    \"hive.metastore.uris\", \n",
    "    \"thrift://localhost:9083\", \n",
    "    conf = SparkConf()\n",
    ").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0863118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the pandas df to a df in the current spark session\n",
    "df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "185e6a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/29 15:38:30 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/10/29 15:38:31 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/10/29 15:38:36 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/10/29 15:38:36 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hduser@127.0.1.1\n",
      "23/10/29 15:38:42 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/10/29 15:38:43 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/10/29 15:38:43 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/10/29 15:38:43 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/10/29 15:38:43 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "# Save as a new table within the hive db\n",
    "df.write.saveAsTable('sentiment_over_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "662ae8b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|      full_date|   sentiment_score|\n",
      "+---------------+------------------+\n",
      "|Mon Apr 06 2009|0.9989890456199646|\n",
      "|Mon Apr 06 2009|0.9827452301979065|\n",
      "|Mon Apr 06 2009|0.9769710898399353|\n",
      "|Mon Apr 06 2009| 0.996757447719574|\n",
      "|Mon Apr 06 2009|0.9962053894996643|\n",
      "+---------------+------------------+\n",
      "\n",
      "+---------------+------------------+\n",
      "|      full_date|   sentiment_score|\n",
      "+---------------+------------------+\n",
      "|Mon Apr 06 2009|0.9989890456199646|\n",
      "|Mon Apr 06 2009|0.9827452301979065|\n",
      "|Mon Apr 06 2009|0.9769710898399353|\n",
      "|Mon Apr 06 2009| 0.996757447719574|\n",
      "|Mon Apr 06 2009|0.9962053894996643|\n",
      "+---------------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check the first 5 rows of the stored data\n",
    "df_load = spark.sql('SELECT * FROM sentiment_over_time limit 5')\n",
    "df_load.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60cd1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_load.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
